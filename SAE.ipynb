{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, expansion_factor: float = 16):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = int(input_dim * expansion_factor)\n",
    "        self.decoder = nn.Linear(self.latent_dim, input_dim, bias=True)\n",
    "        self.encoder = nn.Linear(input_dim, self.latent_dim, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        encoded = F.relu(self.encoder(x))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "     \n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return F.relu(self.encoder(x))\n",
    "            \n",
    "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return self.decoder(x)\n",
    "     \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path: str, input_dim: int, expansion_factor: float = 16, device: str = \"cuda\") -> \"SparseAutoencoder\":\n",
    "        model = cls(input_dim=input_dim, expansion_factor=expansion_factor)\n",
    "        state_dict = torch.load(path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download, notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = hf_hub_download(\n",
    "    repo_id=\"qresearch/Llama-3.2-1B-Instruct-SAE-l9\",\n",
    "    filename=\"Llama-3.2-1B-Instruct-SAE-l9.pt\",\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "sae = SparseAutoencoder.from_pretrained(\n",
    "    path=file_path,\n",
    "    input_dim=2048,\n",
    "    expansion_factor=16,\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 22 Jan 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello, how are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I'm doing well, thank you for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm here to help you with any questions or topics you'd like to discuss. How about you\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_residual_activations(model, target_layer, inputs):\n",
    "    target_act = None\n",
    "    def gather_target_act_hook(mod, inputs, outputs):\n",
    "        nonlocal target_act\n",
    "        target_act = inputs[0]  # Get residual stream from layer input\n",
    "        return outputs\n",
    "        \n",
    "    handle = model.model.layers[target_layer].register_forward_hook(gather_target_act_hook)\n",
    "    with torch.no_grad():\n",
    "        _ = model(inputs)\n",
    "    handle.remove()\n",
    "    return target_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_act = gather_residual_activations(model, 9, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_acts = sae.encode(target_act.to(torch.float32))\n",
    "recon = sae.decode(sae_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained: 0.999\n"
     ]
    }
   ],
   "source": [
    "var_explained = 1 - torch.mean((recon - target_act.to(torch.float32)) ** 2) / torch.var(target_act.to(torch.float32))\n",
    "print(f\"Variance explained: {var_explained:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Roleplay as a pirate\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Yarr, I'll be speakin' like a true seafarer from here on out! Got me sea legs ready and me vocabulary set to proper pirate speak. What can I help ye with, me hearty?\"},\n",
    "    ],\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top activated features during pirate speech:\n",
      "Feature 8672: 0.294\n",
      "Feature 17264: 0.098\n",
      "Feature 16690: 0.097\n",
      "Feature 4529: 0.094\n",
      "Feature 13554: 0.083\n",
      "Feature 19564: 0.067\n",
      "Feature 19652: 0.063\n",
      "Feature 15174: 0.058\n",
      "Feature 14926: 0.053\n",
      "Feature 26442: 0.049\n",
      "Feature 26436: 0.048\n",
      "Feature 20622: 0.047\n",
      "Feature 24020: 0.043\n",
      "Feature 763: 0.038\n",
      "Feature 16229: 0.037\n",
      "Feature 7236: 0.035\n",
      "Feature 12987: 0.034\n",
      "Feature 5232: 0.032\n",
      "Feature 9887: 0.031\n",
      "Feature 30199: 0.029\n",
      "\n",
      "Activation patterns across tokens:\n",
      "\n",
      "Token: ĊĊ\n",
      "  Feature 8672: 0.692\n",
      "  Feature 19564: 0.256\n",
      "  Feature 19652: 0.218\n",
      "  Feature 16229: 1.039\n",
      "\n",
      "Token: Y\n",
      "  Feature 8672: 0.590\n",
      "  Feature 763: 1.219\n",
      "  Feature 16229: 0.266\n",
      "\n",
      "Token: arr\n",
      "  Feature 8672: 0.322\n",
      "  Feature 16690: 0.273\n",
      "  Feature 13554: 0.268\n",
      "  Feature 763: 0.509\n",
      "\n",
      "Token: ,\n",
      "  Feature 8672: 0.479\n",
      "  Feature 19652: 0.215\n",
      "  Feature 16229: 0.288\n",
      "\n",
      "Token: ĠI\n",
      "  Feature 8672: 0.506\n",
      "\n",
      "Token: 'll\n",
      "  Feature 20622: 1.617\n",
      "\n",
      "Token: Ġbe\n",
      "  Feature 8672: 0.402\n",
      "  Feature 17264: 0.234\n",
      "  Feature 20622: 0.567\n",
      "\n",
      "Token: Ġspeak\n",
      "  Feature 8672: 0.371\n",
      "  Feature 4529: 1.998\n",
      "  Feature 24020: 0.292\n",
      "\n",
      "Token: in\n",
      "  Feature 8672: 0.525\n",
      "  Feature 17264: 0.594\n",
      "\n",
      "Token: '\n",
      "  Feature 8672: 0.253\n",
      "  Feature 4529: 1.206\n",
      "\n",
      "Token: Ġlike\n",
      "  Feature 8672: 0.266\n",
      "  Feature 30199: 0.318\n",
      "\n",
      "Token: Ġa\n",
      "  Feature 8672: 0.465\n",
      "  Feature 12987: 0.221\n",
      "  Feature 30199: 0.496\n",
      "\n",
      "Token: Ġtrue\n",
      "  Feature 8672: 0.464\n",
      "  Feature 24020: 0.368\n",
      "  Feature 12987: 0.291\n",
      "  Feature 30199: 0.301\n",
      "\n",
      "Token: Ġse\n",
      "  Feature 8672: 0.239\n",
      "  Feature 13554: 0.507\n",
      "  Feature 26436: 1.099\n",
      "\n",
      "Token: af\n",
      "  Feature 8672: 0.228\n",
      "  Feature 13554: 1.269\n",
      "  Feature 19652: 0.216\n",
      "  Feature 14926: 0.239\n",
      "  Feature 26436: 0.369\n",
      "\n",
      "Token: arer\n",
      "  Feature 8672: 0.531\n",
      "  Feature 19652: 0.236\n",
      "  Feature 14926: 0.580\n",
      "  Feature 24020: 0.212\n",
      "\n",
      "Token: Ġfrom\n",
      "  Feature 8672: 0.460\n",
      "  Feature 19652: 0.201\n",
      "  Feature 9887: 1.430\n",
      "\n",
      "Token: Ġout\n",
      "  Feature 17264: 0.265\n",
      "\n",
      "Token: !\n",
      "  Feature 8672: 0.509\n",
      "\n",
      "Token: ĠGot\n",
      "  Feature 8672: 0.468\n",
      "\n",
      "Token: Ġme\n",
      "  Feature 8672: 0.646\n",
      "  Feature 17264: 1.145\n",
      "  Feature 12987: 0.206\n",
      "\n",
      "Token: Ġsea\n",
      "  Feature 8672: 0.294\n",
      "  Feature 13554: 1.224\n",
      "  Feature 26436: 0.758\n",
      "\n",
      "Token: Ġlegs\n",
      "  Feature 8672: 0.253\n",
      "\n",
      "Token: Ġready\n",
      "  Feature 8672: 0.349\n",
      "  Feature 15174: 0.383\n",
      "  Feature 26442: 2.098\n",
      "\n",
      "Token: Ġand\n",
      "  Feature 8672: 0.391\n",
      "  Feature 19564: 0.220\n",
      "  Feature 15174: 0.237\n",
      "\n",
      "Token: Ġme\n",
      "  Feature 8672: 0.526\n",
      "  Feature 17264: 0.637\n",
      "  Feature 19564: 0.257\n",
      "  Feature 12987: 0.314\n",
      "\n",
      "Token: Ġvocabulary\n",
      "  Feature 8672: 0.333\n",
      "  Feature 19564: 0.302\n",
      "\n",
      "Token: Ġset\n",
      "  Feature 7236: 1.588\n",
      "\n",
      "Token: Ġproper\n",
      "  Feature 8672: 0.330\n",
      "  Feature 12987: 0.268\n",
      "\n",
      "Token: Ġpirate\n",
      "  Feature 8672: 0.342\n",
      "  Feature 13554: 0.326\n",
      "  Feature 19564: 0.249\n",
      "  Feature 15174: 0.231\n",
      "  Feature 14926: 0.729\n",
      "\n",
      "Token: Ġspeak\n",
      "  Feature 8672: 0.205\n",
      "  Feature 4529: 1.059\n",
      "\n",
      "Token: .\n",
      "  Feature 8672: 0.449\n",
      "\n",
      "Token: ĠWhat\n",
      "  Feature 24020: 0.288\n",
      "\n",
      "Token: ĠI\n",
      "  Feature 8672: 0.274\n",
      "\n",
      "Token: Ġye\n",
      "  Feature 17264: 0.884\n",
      "  Feature 24020: 0.281\n",
      "\n",
      "Token: ,\n",
      "  Feature 16690: 0.289\n",
      "\n",
      "Token: Ġme\n",
      "  Feature 8672: 0.245\n",
      "  Feature 17264: 0.252\n",
      "  Feature 16690: 1.998\n",
      "\n",
      "Token: Ġhearty\n",
      "  Feature 8672: 0.242\n",
      "  Feature 16690: 1.585\n",
      "  Feature 19652: 0.226\n",
      "  Feature 14926: 0.405\n",
      "\n",
      "Token: ?\n",
      "  Feature 8672: 0.250\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Get activations\n",
    "target_act = gather_residual_activations(model, 9, inputs)\n",
    "sae_acts = sae.encode(target_act.to(torch.float32))\n",
    "\n",
    "# Get token IDs and decode them for reference\n",
    "tokens = inputs[0].cpu().numpy()\n",
    "token_texts = tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "# Find which tokens are part of the assistant's response\n",
    "token_ids = inputs[0].cpu().numpy()\n",
    "is_special = (token_ids >= 128000) & (token_ids <= 128255)\n",
    "special_positions = np.where(is_special)[0]\n",
    "\n",
    "assistant_start = special_positions[-2] + 1\n",
    "assistant_tokens = slice(assistant_start, None)\n",
    "\n",
    "# Get activation statistics for assistant's response\n",
    "assistant_activations = sae_acts[0, assistant_tokens]\n",
    "mean_activations = assistant_activations.mean(dim=0)\n",
    "\n",
    "# Find top activated features during pirate speech\n",
    "num_top_features = 20\n",
    "top_features = mean_activations.topk(num_top_features)\n",
    "\n",
    "print(\"Top activated features during pirate speech:\")\n",
    "for idx, value in zip(top_features.indices, top_features.values):\n",
    "    print(f\"Feature {idx}: {value:.3f}\")\n",
    "\n",
    "# Look at how these features activate across different tokens\n",
    "print(\"\\nActivation patterns across tokens:\")\n",
    "for i, (token, acts) in enumerate(zip(token_texts[assistant_tokens], assistant_activations)):\n",
    "    top_acts = acts[top_features.indices]\n",
    "    if top_acts.max() > 0.2:  # Only show tokens with significant activation\n",
    "        print(f\"\\nToken: {token}\")\n",
    "        for feat_idx, act_val in zip(top_features.indices, top_acts):\n",
    "            if act_val > 0.2:  # Threshold for \"active\" features\n",
    "                print(f\"  Feature {feat_idx}: {act_val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quentin/dev/simple_control_vectors/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/quentin/dev/simple_control_vectors/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original generation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 22 Jan 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello, how are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I'm doing well, thank you for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm here to help you with any questions or topics you'd like to discuss. How about you? How's your day going so far?<|eot_id|>\n",
      "\n",
      "Generation with modified feature:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 22 Jan 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hello, how are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "*ahh, easy to ask a question. *sigh* Ah, I'm doin' alright, mate. The sun's been shinin' bright, and the coffee's got its cup. *grumble* That's the life. Now, what's the business?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "def generate_with_intervention(\n",
    "    model, \n",
    "    tokenizer, \n",
    "    sae,\n",
    "    messages: list[dict],\n",
    "    feature_idx: int,\n",
    "    intervention: float = 3.0,\n",
    "    target_layer: int = 9,\n",
    "    max_new_tokens: int = 50\n",
    "):\n",
    "    modified_activations = None\n",
    "    \n",
    "    def intervention_hook(module, inputs, outputs):\n",
    "        nonlocal modified_activations\n",
    "        activations = inputs[0]\n",
    "        \n",
    "        features = sae.encode(activations.to(torch.float32))\n",
    "        reconstructed = sae.decode(features)\n",
    "        error = activations.to(torch.float32) - reconstructed\n",
    "        \n",
    "        features[:, :, feature_idx] += intervention\n",
    "        \n",
    "        modified = sae.decode(features) + error\n",
    "        modified_activations = modified\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def output_hook(module, inputs, outputs):\n",
    "        nonlocal modified_activations\n",
    "        if modified_activations is not None:\n",
    "            return (modified_activations,) + outputs[1:] if len(outputs) > 1 else (modified_activations,)\n",
    "        return outputs\n",
    "    \n",
    "    handles = [\n",
    "        model.model.layers[target_layer].register_forward_hook(intervention_hook),\n",
    "        model.model.layers[target_layer].register_forward_hook(output_hook)\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        input_tokens = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_tokens,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False  # Use greedy decoding for consistency\n",
    "        )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0])\n",
    "        \n",
    "    finally:\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "]\n",
    "feature_to_modify = 8672\n",
    "\n",
    "print(\"Original generation:\")\n",
    "input_tokens = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "outputs = model.generate(input_tokens, max_new_tokens=100, do_sample=False)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "print(\"\\nGeneration with modified feature:\")\n",
    "modified_text = generate_with_intervention(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    sae=sae,\n",
    "    messages=messages,\n",
    "    feature_idx=feature_to_modify,\n",
    "    intervention=3,\n",
    "    target_layer=9,\n",
    "    max_new_tokens=100\n",
    ")\n",
    "print(modified_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
